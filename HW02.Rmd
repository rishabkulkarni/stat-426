---
title: "HW02"
author: "Rishab Kulkarni"
date: "1/31/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**#1**

The probability distribution of the random vector $(X_1,X_2,...X_5)$
given $\sum_{i=1}^{n} x_i=20$ is a multinomial distribution with $(n,{\pi_i})$

{$\pi_i = \pi_i/\sum_{i=1}^{n}\mu_i$}, so the distribution is as follows:
$\pi_1 = 1/15, \pi_2 = 2/15, \pi_3 = 1/5, \pi_4 = 4/15, \pi_5 = 1/3$

The probability distribution of $X_1$ given $\sum_{i=1}^{n} x_i=20$
is a binomial distribution with $(n=20,\pi_1 = 1/15)$.

**#2**

The vector $(N_1, N_2,...N_5)$ has a multinomial distribution, where
the marginal distribution of each $N_i$ is a binomial distribution.

$E(N_i)=n*\pi_i$, and $\pi_i$ would be 1/5 as there is a 1/5 chance
of a random variable falling into one of the 5 intervals 
$(\frac{k-1}{5},\frac{k}{5})$ for $k = 1,2,...5$. Thus, the expected
value for each $N_i= 100*1/5 = 20$. As such, $E(N)=(20,20,20,20,20)$

The $cov(N_j,N_k)= -n\pi_j\pi_k$. As each $\pi_i=1/5$, this value
should be $-100*1/5*1/5 = -4$, which is for the off-diagonal cells
of the covariance matrix. The diagonal cells of the covariance 
matrix would be $var(N_i)$, where each $N_i$ follows a binomial
distribution. The variance of binomial distribution is $np(1-p)$,
which would be $100*1/5*4/5=16$. This would be the value for the
diagonal cells of the matrix.

$\begin{bmatrix}
16 & -4 & -4 & -4 & -4\\
-4 & 16 & -4 & -4 & -4\\
-4 & -4 & 16 & -4 & -4\\
-4 & -4 & -4 & 16 & -4\\
-4 & -4 & -4 & -4 & 16
\end{bmatrix}$

**#3**.

```{r}
n <- c(14,18,19,16,33)

# chi-square dist. with 4 deg. freedom
# asymptotic distribution

chi_asymp <- chisq.test(n, p=rep(1/5,5))
t_stat <- chi_asymp$statistic
asymp_pval <- chi_asymp$p.value

t_stat
asymp_pval

# simulation


chi_exact <- chisq.test(n, p=rep(1/5,5), simulate.p.value=TRUE, B=10000)
t_stat_exact <- chi_exact$statistic
exact_pval <- chi_exact$p.value


t_stat_exact
# exact p-value
exact_pval
```

The test statistic remains 11.3, and the p-value only slightly changes.
This implies that the chi-square approximation is pretty good.

**#4**.
```{r}
# Likelihood-Ratio test
# n is the observed values

exp = sum(n) * rep(1/5, 5)
lik.teststat = (G.squared <- 2*sum(n*ifelse(n>0,log(n/exp),0)))
lik.pval= 1-pchisq(G.squared, df=5-1)

lik.teststat
lik.pval

```

```{r}
# Likelihood-Ratio test exact pval
# simulation

reps= 10000
size= 100
teststats= 1:reps
p = rep(1/5,5)
exp=size*p


for(i in 1:reps){
  obs = rmultinom(1,size,p)
  teststats[i] =(G.squared <- 2 *sum(obs*ifelse(obs>0,log(obs/exp), 0)))
}

# proportion of simulated test stats greater than LR-test stat from observed data 
# exact p-value
g=0
for(t in teststats){
  if(t > lik.teststat){
    g=g+1
  }
}
g/reps

```

The exact p-value is 0.0383, and can be around 0.04 under some conditions.
This is close to the LR-test p-value of 0.0375 using the asymptotic
distribution. In other words, the p-value only slightly changes which
indicates that the chi-square approximation is good.

**#5**

```{r}
# Benford's law
d <- 1:9
prob <- log10(1+1/d)

reps=10000
size=10;teststats=1:reps;expected=size*prob 

for(i in 1:reps){
observed=rmultinom(1,size,prob) 
teststats[i]=sum((observed-expected)^2/expected)
}
## now plot cdf of test stats beside chi-sq 8 cdf
teststats=sort(teststats) 
exact.cdf=c(1:reps)
for(i in 1:reps){
  exact.cdf[i]=mean(teststats<=teststats[i])
}
Chi.cdf=pchisq(teststats,df=9)
maxstat=max(teststats) 

# plotting exact distribution
plot(teststats,exact.cdf,xlim=c(0,maxstat),ylim=c(0,1),xlab="x",ylab="Fx",col="blue",type="l")
par(new=TRUE) 

# plotting asymptotic cumulative distribution
plot(teststats,Chi.cdf,xlim=c(0,maxstat),ylim=c(0,1),xlab="x",ylab="Fx",col="red",type ="l")
```

The exact and asymptotic cumulative distribution function don't differ
that much. For x < 20, they begin different as the exact distribution
has higher Fx than the asymptotic distribution function. However, for
x >= 20, both functions level off at Fx = 1.

In the worst case, these functions don't differ significantly.

